Focus: GBOW, Skip-gram, Negative Sampling, Hierarchical Softmax, Glove

Notes:
1. Mini-batch SGD具有优点:
a) 通过平均值，减少梯度估计的噪音
b) 在GPU上并行化运算，加快运算速度

2. 去掉the, and, that, of这一部分可以使词向量效果更好

3. 每行代表一个单词的词向量，点乘后得到的分数通过softmax映射为概率分布，并且我们得到的概率分布是对于该中心词而言的上下文中单词的概率分布，
该分布于上下文所在的具体位置无关，所以在每个位置的预测都是一样的。 即不关心窗口内上下文具体的position，这点是后来很多论文的改进方向。

4. Skip-grams (SG)
输入中心词并预测上下文中的单词
Continuous Bag of Words (CBOW)
输入上下文中的单词并预测中心词

5. Negative Sampling 负采样方法

使用一个 true pair (中心词及其上下文窗口中的词)与几个 noise pair (中心词与随机词搭配) 形成的样本，训练二元逻辑回归

6. 通过 3/4次方，相对减少常见单词的频率，增大稀有词的概率

7. 如果使向量点积等于共现概率的对数，那么向量差异变成了共现概率的比率。使用平方误差促使点积尽可能得接近共现概率的对数。

8. Hierarchical softmax 使用一个二叉树来表示词表中的所有词。树中的每个叶结点都是一个单词，而且只有一条路径从根结点到叶结点。在这个模型中，没有词的输出表示。
相反，图的每个节点（根节点和叶结点除外）与模型要学习的向量相关联。单词作为输出单词的概率定义为从根随机游走到单词所对应的叶的概率。计算成本降为对数级别。

9. 在实际中，hierarchical softmax 对低频词往往表现得更好，负采样对高频词和较低维度向量表现得更好。

TODO： Glove还没完全整明白。wordtovec相关高频面试题待整理。
